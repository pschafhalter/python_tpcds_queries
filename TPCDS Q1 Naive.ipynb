{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TPCDS Q1 in Python\n",
    "\n",
    "```SQL\n",
    "WITH customer_total_return AS\n",
    "( SELECT\n",
    "    sr_customer_sk AS ctr_customer_sk,\n",
    "    sr_store_sk AS ctr_store_sk,\n",
    "    sum(sr_return_amt) AS ctr_total_return\n",
    "  FROM store_returns, date_dim\n",
    "  WHERE sr_returned_date_sk = d_date_sk AND d_year = 2000\n",
    "  GROUP BY sr_customer_sk, sr_store_sk)\n",
    "SELECT c_customer_id\n",
    "FROM customer_total_return ctr1, store, customer\n",
    "WHERE ctr1.ctr_total_return >\n",
    "  (SELECT avg(ctr_total_return) * 1.2\n",
    "  FROM customer_total_return ctr2\n",
    "  WHERE ctr1.ctr_store_sk = ctr2.ctr_store_sk)\n",
    "  AND s_store_sk = ctr1.ctr_store_sk\n",
    "  AND s_state = 'TN'\n",
    "  AND ctr1.ctr_customer_sk = c_customer_sk\n",
    "ORDER BY c_customer_id\n",
    "LIMIT 100\n",
    "```\n",
    "High level plan for query:\n",
    "1. Filter indivudial tables if possible (typically `O(N)`)\n",
    "    - operations that combine 2 tables (e.g. through cartesian product) are expensive, so we want these to be as small as possible\n",
    "2. Combine pairs of tables.\n",
    "3. Treat combined pairs of tables as individual tables and return to (1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_header(table_name):\n",
    "    with open(\"headers/\" + table_name, \"r\") as f:\n",
    "        return f.read().split(\"\\n\")\n",
    "\n",
    "def read_table(table_name):\n",
    "    header = read_header(table_name)\n",
    "    table = pd.read_csv(\"data/\" + table_name + \".csv\", names=header, delimiter='|')\n",
    "    table.tail()\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load tables of interest\n",
    "store_returns = read_table(\"store_returns\")\n",
    "date_dim = read_table(\"date_dim\")\n",
    "store = read_table(\"store\")\n",
    "customer = read_table(\"customer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def filtered_cartesian_product(A, B, filter_fn):\n",
    "    \"\"\"Returns the rows of the cartesian product of A, B for which filter_fn is True.\n",
    "    \n",
    "    Used for SELECTs from multiple tables\n",
    "    Has O(N M) runtime where N is the number of rows in A and M is the number of rows in B.\n",
    "    \n",
    "    Params:\n",
    "        - A: a table (pandas dataframe object)\n",
    "        - B: a table (pandas dataframe object)\n",
    "        - filter_fn(a, b): takes in a potential pair of rows a in A, b in B and returns true\n",
    "            if concatenating them forms a valid new row\n",
    "            \n",
    "    Returns:\n",
    "        pandas Series object.\n",
    "    \"\"\"\n",
    "    # Note that B.apply could be replaced with a map function over B's rows\n",
    "    # Get a mapping from A's rows to sets of valid rows in B\n",
    "    valid_row_table_pairs = map(\n",
    "        lambda pair: (pair[1], B[B.apply(lambda b: filter_fn(pair[1], b), axis=1)]),\n",
    "        A.iterrows() # returns index, row pair\n",
    "    )\n",
    "    # Transform each A_row -> B_valid_rows mapping to a list of concatenated rows\n",
    "    concat_row_table_pair = lambda row_tbl: map(\n",
    "        lambda idx_row: pd.concat([row_tbl[0], idx_row[1]]),\n",
    "        row_tbl[1].iterrows())\n",
    "    row_pair_lists = map(concat_row_table_pair, valid_row_table_pairs)\n",
    "    # Flatten the list of lists\n",
    "    valid_rows = [i for lst in row_pair_lists for i in lst]\n",
    "    # Transform list of lists into pandas object\n",
    "    return pd.concat(valid_rows, axis=1).transpose().reset_index()\n",
    "\n",
    "def merge_tables(A, B, a_column, b_column):\n",
    "    \"\"\"Merges 2 tables where a_column and b_column are equal.\n",
    "    \n",
    "    Assumes that a_column and b_column contain unique keys.\n",
    "    This is *much* faster than filtered_cartesian product.\n",
    "    However, it can be replaced with a map/reduce:\n",
    "        1. map over rows of A\n",
    "        2. select row with matching key in B\n",
    "        3. return concatenation of row from A with row from B\n",
    "        4. reduce list of concatenated rows by converting to dataframe\n",
    "    \n",
    "    Params:\n",
    "        - A: a table (pandas dataframe object)\n",
    "        - B: a table (pandas dataframe object)\n",
    "        - a_column: string name of column of keys in A\n",
    "        - b_column: string name of column of keys in B\n",
    "    \n",
    "    Returns:\n",
    "        pandas dataframe\n",
    "        \n",
    "    Note:\n",
    "        - The returned dataframe does not contain a column with the label\n",
    "            b_column.\n",
    "    \"\"\"\n",
    "    B_renamed  = B.rename(columns={b_column: a_column}, inplace=False)\n",
    "    return pd.merge(A, B_renamed, on=a_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct `customer_total_return`\n",
    "```SQL\n",
    "SELECT\n",
    "    sr_customer_sk AS ctr_customer_sk,\n",
    "    sr_store_sk AS ctr_store_sk,\n",
    "    sum(sr_return_amt) AS ctr_total_return\n",
    "  FROM store_returns, date_dim\n",
    "  WHERE sr_returned_date_sk = d_date_sk AND d_year = 2000\n",
    "  GROUP BY sr_customer_sk, sr_store_sk)\n",
    "SELECT c_customer_id\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Filter indivual tables if possible\n",
    "date_dim_filtered = date_dim[date_dim[\"d_year\"] == 2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Merge date_dim and store_returns\n",
    "ctr_merged = merge_tables(date_dim_filtered, store_returns, \"d_date_sk\", \"sr_returned_date_sk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Group and apply aggregate function\n",
    "ctr_grouped = ctr_merged.groupby([\"sr_customer_sk\", \"sr_store_sk\"])\n",
    "ctr_summed = ctr_grouped.agg({\"sr_return_amt\": np.sum})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       ctr_customer_sk  ctr_store_sk  ctr_total_return\n",
      "0                  5.0           2.0           4260.39\n",
      "1                  5.0          10.0            328.85\n",
      "2                  6.0           4.0           2118.22\n",
      "3                 16.0           2.0           1411.41\n",
      "4                 18.0           2.0              0.00\n",
      "5                 18.0           4.0           2137.09\n",
      "6                 19.0           4.0            253.29\n",
      "7                 20.0           8.0              9.40\n",
      "8                 24.0           1.0           1462.44\n",
      "9                 24.0          10.0             67.22\n",
      "10                26.0           1.0           1040.85\n",
      "11                31.0           2.0           1771.50\n",
      "12                32.0          10.0            174.80\n",
      "13                35.0           1.0            536.21\n",
      "14                35.0           2.0           6631.56\n",
      "15                36.0           2.0            835.20\n",
      "16                38.0           2.0           6609.68\n",
      "17                44.0           7.0           2731.70\n",
      "18                45.0          10.0            188.79\n",
      "19                48.0           7.0            779.54\n",
      "20                52.0           4.0             19.46\n",
      "21                52.0          10.0           3063.32\n",
      "22                53.0          10.0             76.96\n",
      "23                57.0           8.0           1331.40\n",
      "24                57.0          10.0             15.66\n",
      "25                58.0           1.0            526.88\n",
      "26                58.0           4.0           2386.50\n",
      "27                61.0           7.0            470.32\n",
      "28                61.0           8.0            271.44\n",
      "29                63.0           4.0             23.16\n",
      "...                ...           ...               ...\n",
      "49903          99942.0          10.0             22.32\n",
      "49904          99943.0           1.0             84.72\n",
      "49905          99943.0           8.0           1008.44\n",
      "49906          99948.0           2.0              7.16\n",
      "49907          99952.0           7.0            360.72\n",
      "49908          99953.0           7.0           2764.28\n",
      "49909          99954.0           4.0            331.68\n",
      "49910          99955.0           8.0            594.00\n",
      "49911          99962.0           8.0           1886.66\n",
      "49912          99964.0           2.0            135.60\n",
      "49913          99967.0           1.0            258.96\n",
      "49914          99967.0           8.0            146.34\n",
      "49915          99971.0           1.0            156.06\n",
      "49916          99974.0          10.0            344.59\n",
      "49917          99975.0           2.0            448.08\n",
      "49918          99976.0           4.0           1231.56\n",
      "49919          99979.0          10.0            567.54\n",
      "49920          99981.0           1.0            260.40\n",
      "49921          99983.0           4.0           3990.72\n",
      "49922          99983.0           7.0           2432.55\n",
      "49923          99984.0           4.0            688.25\n",
      "49924          99985.0           2.0           2374.29\n",
      "49925          99986.0           7.0           2814.24\n",
      "49926          99991.0           8.0               NaN\n",
      "49927          99993.0           1.0            100.24\n",
      "49928          99993.0           8.0            218.30\n",
      "49929          99995.0           2.0            387.43\n",
      "49930          99995.0           4.0            543.51\n",
      "49931          99995.0           7.0            440.68\n",
      "49932         100000.0          10.0            504.42\n",
      "\n",
      "[49933 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Fix indexing\n",
    "customer_total_return = ctr_summed.reset_index()\n",
    "customer_total_return.rename(columns={\"sr_customer_sk\": \"ctr_customer_sk\",\n",
    "                                  \"sr_store_sk\": \"ctr_store_sk\",\n",
    "                                  \"sr_return_amt\": \"ctr_total_return\"\n",
    "                                 }, inplace=True)\n",
    "print(customer_total_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Filter on customer_total_return\n",
    "ctr1, ctr2 = customer_total_return, customer_total_return\n",
    "\n",
    "# Could cache mean to speed up.\n",
    "def filter_avg(r):\n",
    "    ctr2_filtered = ctr2[ctr2[\"ctr_store_sk\"] == r[\"ctr_store_sk\"]]\n",
    "    avg = ctr2_filtered[\"ctr_total_return\"].mean()\n",
    "    return r[\"ctr_total_return\"] > 1.2 * avg\n",
    "\n",
    "ctr1_filtered = ctr1[ctr1.apply(filter_avg, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "store_filtered = store[store[\"s_state\"] == \"TN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge ctr1 and store\n",
    "store_ctr1 = merge_tables(store_filtered, ctr1_filtered, \"s_store_sk\", \"ctr_store_sk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Merge with customer\n",
    "store_ctr1_customer = merge_tables(store_ctr1, customer, \"ctr_customer_sk\", \"c_customer_sk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SELECT c_customer_id, ORDER BY c_customer_id LIMIT 100\n",
    "result = store_ctr1_customer.sort_values(\"c_customer_id\")[\"c_customer_id\"][0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 366 entries, 36523 to 36888\n",
      "Data columns (total 29 columns):\n",
      "d_date_sk              366 non-null int64\n",
      "d_date_id (B)          366 non-null object\n",
      "d_date                 366 non-null object\n",
      "d_month_seq            366 non-null int64\n",
      "d_week_seq             366 non-null int64\n",
      "d_quarter_seq          366 non-null int64\n",
      "d_year                 366 non-null int64\n",
      "d_dow                  366 non-null int64\n",
      "d_moy                  366 non-null int64\n",
      "d_dom                  366 non-null int64\n",
      "d_qoy                  366 non-null int64\n",
      "d_fy_year              366 non-null int64\n",
      "d_fy_quarter_seq       366 non-null int64\n",
      "d_fy_week_seq          366 non-null int64\n",
      "d_day_name             366 non-null object\n",
      "d_quarter_name         366 non-null object\n",
      "d_holiday              366 non-null object\n",
      "d_weekend              366 non-null object\n",
      "d_following_holiday    366 non-null object\n",
      "d_first_dom            366 non-null int64\n",
      "d_last_dom             366 non-null int64\n",
      "d_same_day_ly          366 non-null int64\n",
      "d_same_day_lq          366 non-null int64\n",
      "d_current_day          366 non-null object\n",
      "d_current_week         366 non-null object\n",
      "d_current_month        366 non-null object\n",
      "d_current_quarter      366 non-null object\n",
      "d_current_year         366 non-null object\n",
      "                       0 non-null float64\n",
      "dtypes: float64(1), int64(16), object(12)\n",
      "memory usage: 85.8+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(date_dim[date_dim[\"d_year\"] == 2000].info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
